{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6104588a-0b31-4482-b157-193629f6ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Initializing Spark Session...\n",
      "    (If this takes >2 mins, it is downloading MongoDB connectors)\n",
      "âœ… Spark Session Active: v3.4.3\n",
      "\n",
      ">>> 1. READING FROM MONGODB (aci.articles)...\n",
      "   - Loaded 272 raw articles.\n",
      "\n",
      ">>> 2. TRANSFORMING DATA...\n",
      "\n",
      ">>> 3. LOADING TO DATA WAREHOUSE (aci.fact_publications)...\n",
      "   - Success. Data stored in MongoDB.\n",
      "\n",
      ">>> 4. GENERATING HDFS STRUCTURE AT: C:\\Users\\Dell\\Desktop\\Projet_BI\\S2_ApacheAnalysis\\hdfs_data\n",
      "   - HDFS Folder Structure Created âœ…\n",
      "\n",
      "========================================\n",
      "        SPARK ANALYSIS RESULTS        \n",
      "========================================\n",
      "\n",
      "ðŸ“Š 1. Top Countries by Impact Factor:\n",
      "+-------+-----------------+---------+\n",
      "|country|       avg_impact|pub_count|\n",
      "+-------+-----------------+---------+\n",
      "|  China|9.286122448979588|       49|\n",
      "|     UK|        8.7134375|       32|\n",
      "| France|8.413749999999999|       24|\n",
      "|    USA|8.362153846153845|       65|\n",
      "|Morocco|7.951428571428571|       14|\n",
      "+-------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "âœ… ETL COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# ==========================================\n",
    "# 1. AUTO-INSTALL REQUIREMENTS ON PYTHON 3.11\n",
    "# ==========================================\n",
    "# We define the path to your Python 3.11 executable based on your screenshot\n",
    "PYTHON_311_PATH = r\"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\"\n",
    "\n",
    "# If we are NOT currently running on Python 3.11, we warn the user\n",
    "if sys.executable != PYTHON_311_PATH:\n",
    "    print(f\"âš ï¸ WARNING: Current Kernel is {sys.version.split()[0]}\")\n",
    "    print(f\"   Spark needs Python 3.11. Configuring Spark to use: {PYTHON_311_PATH}\")\n",
    "\n",
    "    # Ensure pyspark is installed on the 3.11 environment specifically\n",
    "    print(\"   Verifying libraries on Python 3.11...\")\n",
    "    try:\n",
    "        subprocess.check_call([PYTHON_311_PATH, \"-m\", \"pip\", \"install\", \"pyspark\", \"mongo-spark-connector\", \"pymongo\", \"pandas\"], \n",
    "                              stdout=subprocess.DEVNULL)\n",
    "        print(\"   âœ… Libraries verified on Python 3.11.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Could not auto-install libraries: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ENVIRONMENT CONFIGURATION\n",
    "# ==========================================\n",
    "# Paths verified from your previous successes\n",
    "os.environ['JAVA_HOME'] = r\"C:\\jdk-11\"\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "\n",
    "# CRITICAL FIX: Force Spark to use Python 3.11 worker even if the notebook is 3.13\n",
    "os.environ['PYSPARK_PYTHON'] = PYTHON_311_PATH\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = PYTHON_311_PATH\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, size, split, current_timestamp, desc, avg, count\n",
    "from pyspark.sql.types import StringType, DoubleType, ArrayType, IntegerType\n",
    "\n",
    "# Local folder for temp data (Anti-Freeze fix)\n",
    "warehouse_location = os.path.abspath(\"spark-warehouse\")\n",
    "ivy_location = os.path.abspath(\"spark-ivy\") \n",
    "\n",
    "print(\"\\n>>> Initializing Spark Session...\")\n",
    "print(\"    (If this takes >2 mins, it is downloading MongoDB connectors)\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BlockchainDWBuilder\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.hadoop.home.dir\", r\"C:\\hadoop\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/aci.articles\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/aci.fact_publications\") \\\n",
    "    .config(\"spark.jars.ivy\", ivy_location) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark Session Active: v{spark.version}\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DEFINE UDFS (Simulate BI Data)\n",
    "# ==========================================\n",
    "def simulate_quartile():\n",
    "    return random.choices([\"Q1\", \"Q2\", \"Q3\", \"Q4\"], weights=[25, 35, 25, 15])[0]\n",
    "\n",
    "def simulate_country():\n",
    "    countries = [\"USA\", \"China\", \"India\", \"UK\", \"France\", \"Germany\", \"Morocco\", \"Canada\", \"Japan\"]\n",
    "    weights = [20, 18, 15, 10, 8, 8, 5, 5, 11]\n",
    "    return random.choices(countries, weights=weights)[0]\n",
    "\n",
    "def simulate_impact_score():\n",
    "    return round(random.uniform(0.5, 15.0), 2)\n",
    "\n",
    "def extract_keywords(title):\n",
    "    stopwords = [\"the\", \"for\", \"and\", \"with\", \"based\", \"using\", \"approach\", \"blockchain\", \"analysis\"]\n",
    "    if not title: return []\n",
    "    words = title.lower().replace(\"-\", \" \").split()\n",
    "    return [w.capitalize() for w in words if len(w) > 4 and w not in stopwords]\n",
    "\n",
    "udf_quartile = udf(simulate_quartile, StringType())\n",
    "udf_country = udf(simulate_country, StringType())\n",
    "udf_impact = udf(simulate_impact_score, DoubleType())\n",
    "udf_keywords = udf(extract_keywords, ArrayType(StringType()))\n",
    "\n",
    "# ==========================================\n",
    "# 4. ETL PROCESS\n",
    "# ==========================================\n",
    "print(\"\\n>>> 1. READING FROM MONGODB (aci.articles)...\")\n",
    "try:\n",
    "    raw_df = spark.read.format(\"mongodb\").load()\n",
    "    count_raw = raw_df.count()\n",
    "    print(f\"   - Loaded {count_raw} raw articles.\")\n",
    "    \n",
    "    if count_raw == 0:\n",
    "        print(\"âš ï¸ WARNING: MongoDB collection is empty! Run scraping first.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error reading MongoDB: {e}\")\n",
    "    spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n>>> 2. TRANSFORMING DATA...\")\n",
    "# Clean Authors: \"Name1;\\nName2\" -> Array [\"Name1\", \"Name2\"]\n",
    "df_clean = raw_df.withColumn(\"authors_clean\", split(col(\"authors\"), \";\\\\\\\\n|;\"))\n",
    "\n",
    "# Build Fact Table\n",
    "df_dw = df_clean.withColumn(\"nb_authors\", size(col(\"authors_clean\"))) \\\n",
    "    .withColumn(\"quartile\", udf_quartile()) \\\n",
    "    .withColumn(\"country\", udf_country()) \\\n",
    "    .withColumn(\"impact_score\", udf_impact()) \\\n",
    "    .withColumn(\"citations\", (col(\"impact_score\") * 10).cast(IntegerType())) \\\n",
    "    .withColumn(\"generated_keywords\", udf_keywords(col(\"title\"))) \\\n",
    "    .withColumn(\"etl_timestamp\", current_timestamp())\n",
    "\n",
    "# Drop raw columns\n",
    "df_final = df_dw.drop(\"authors\", \"_id\")\n",
    "\n",
    "print(\"\\n>>> 3. LOADING TO DATA WAREHOUSE (aci.fact_publications)...\")\n",
    "df_final.write.format(\"mongodb\").mode(\"overwrite\").save()\n",
    "print(\"   - Success. Data stored in MongoDB.\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. HDFS ARBORESCENCE (The Deliverable)\n",
    "# ==========================================\n",
    "hdfs_output_path = \"hdfs_data\"\n",
    "print(f\"\\n>>> 4. GENERATING HDFS STRUCTURE AT: {os.path.abspath(hdfs_output_path)}\")\n",
    "\n",
    "# Write to Parquet, partitioned by Country\n",
    "df_final.write.mode(\"overwrite\").partitionBy(\"country\").parquet(hdfs_output_path)\n",
    "print(\"   - HDFS Folder Structure Created âœ…\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. ANALYTICS PREVIEW\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"        SPARK ANALYSIS RESULTS        \")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"\\nðŸ“Š 1. Top Countries by Impact Factor:\")\n",
    "df_final.groupBy(\"country\") \\\n",
    "    .agg(avg(\"impact_score\").alias(\"avg_impact\"), count(\"title\").alias(\"pub_count\")) \\\n",
    "    .orderBy(desc(\"avg_impact\")) \\\n",
    "    .show(5)\n",
    "\n",
    "print(\"\\nâœ… ETL COMPLETE.\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91866311-8f16-4f36-8a30-c3525ff9489c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
