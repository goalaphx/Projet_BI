{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52f28ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initializing Spark Session (Safe Mode)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     30\u001b[39m ivy_location = os.path.abspath(\u001b[33m\"\u001b[39m\u001b[33mspark-ivy\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Create a local cache folder\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>> Initializing Spark Session (Safe Mode)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBlockchainDWBuilder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[1]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.host\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.bindAddress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.warehouse.dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarehouse_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.home.dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mhadoop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars.packages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.mongodb.spark:mongo-spark-connector_2.12:10.2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.mongodb.read.connection.uri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmongodb://localhost:27017/aci.articles\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.mongodb.write.connection.uri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmongodb://localhost:27017/aci.fact_publications\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars.ivy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mivy_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# <--- If it hangs here for >2 mins, it's downloading the JARs\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Spark Session Active: v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Set log level to WARN to reduce noise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:557\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    555\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    560\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:542\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:206\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    209\u001b[39m         master,\n\u001b[32m    210\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         memory_profiler_cls,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:463\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import socketserver\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 0. CRITICAL FIX FOR PYTHON 3.13\n",
    "# ==========================================\n",
    "# Spark 3.x uses a method removed in Python 3.13. This patches it.\n",
    "if not hasattr(socketserver, \"UnixStreamServer\"):\n",
    "    socketserver.UnixStreamServer = socketserver.TCPServer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, size, split, current_timestamp, desc, avg, count\n",
    "from pyspark.sql.types import StringType, DoubleType, ArrayType, IntegerType\n",
    "\n",
    "# ==========================================\n",
    "# 1. ENVIRONMENT CONFIGURATION\n",
    "# ==========================================\n",
    "# FIX: Use 'Progra~1' to avoid spaces in path which crashes Spark on Windows\n",
    "os.environ['JAVA_HOME'] = r\"C:\\Progra~1\\Java\\jdk-11\"\n",
    "os.environ['HADOOP_HOME'] = r\"C:\\hadoop\"\n",
    "\n",
    "# Force Spark to use the current Python kernel\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# --- ANTI-FREEZE CONFIGURATION ---\n",
    "# Define local folders for Spark temp files to prevent permission/path errors\n",
    "warehouse_location = os.path.abspath(\"spark-warehouse\")\n",
    "ivy_location = os.path.abspath(\"spark-ivy\") \n",
    "\n",
    "print(f\">>> JAVA_HOME configured as: {os.environ['JAVA_HOME']}\")\n",
    "print(\">>> Initializing Spark Session (Safe Mode)...\")\n",
    "print(\"    (Note: If this is the first run, it may take 2-3 mins to download MongoDB connectors)\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BlockchainDWBuilder\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.hadoop.home.dir\", r\"C:\\hadoop\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/aci.articles\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/aci.fact_publications\") \\\n",
    "    .config(\"spark.jars.ivy\", ivy_location) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark Session Active: v{spark.version}\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. DEFINE UDFS (Simulating BI Data)\n",
    "# ==========================================\n",
    "def simulate_quartile():\n",
    "    return random.choices([\"Q1\", \"Q2\", \"Q3\", \"Q4\"], weights=[25, 35, 25, 15])[0]\n",
    "\n",
    "def simulate_country():\n",
    "    countries = [\"USA\", \"China\", \"India\", \"UK\", \"France\", \"Germany\", \"Morocco\", \"Canada\", \"Japan\"]\n",
    "    weights = [20, 18, 15, 10, 8, 8, 5, 5, 11]\n",
    "    return random.choices(countries, weights=weights)[0]\n",
    "\n",
    "def simulate_impact_score():\n",
    "    return round(random.uniform(0.5, 15.0), 2)\n",
    "\n",
    "def extract_keywords(title):\n",
    "    stopwords = [\"the\", \"for\", \"and\", \"with\", \"based\", \"using\", \"approach\", \"blockchain\", \"analysis\"]\n",
    "    if not title: return []\n",
    "    words = title.lower().replace(\"-\", \" \").split()\n",
    "    return [w.capitalize() for w in words if len(w) > 4 and w not in stopwords]\n",
    "\n",
    "udf_quartile = udf(simulate_quartile, StringType())\n",
    "udf_country = udf(simulate_country, StringType())\n",
    "udf_impact = udf(simulate_impact_score, DoubleType())\n",
    "udf_keywords = udf(extract_keywords, ArrayType(StringType()))\n",
    "\n",
    "# ==========================================\n",
    "# 3. ETL PROCESS\n",
    "# ==========================================\n",
    "print(\"\\n>>> 1. READING FROM MONGODB...\")\n",
    "try:\n",
    "    raw_df = spark.read.format(\"mongodb\").load()\n",
    "    count_raw = raw_df.count()\n",
    "    print(f\"   - Loaded {count_raw} raw articles.\")\n",
    "    \n",
    "    if count_raw == 0:\n",
    "        print(\"âš ï¸ WARNING: Your MongoDB collection 'aci.articles' is empty!\")\n",
    "        print(\"   Run the scraping scripts first.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error reading MongoDB: {e}\")\n",
    "    # Stop execution safely if Mongo fails\n",
    "    spark.stop()\n",
    "    raise e\n",
    "\n",
    "print(\"\\n>>> 2. TRANSFORMING DATA...\")\n",
    "# Clean Author string \"Name1;\\nName2\" -> Array [\"Name1\", \"Name2\"]\n",
    "df_clean = raw_df.withColumn(\"authors_clean\", split(col(\"authors\"), \";\\\\\\\\n|;\"))\n",
    "\n",
    "# Construct Fact Table\n",
    "df_dw = df_clean.withColumn(\"nb_authors\", size(col(\"authors_clean\"))) \\\n",
    "    .withColumn(\"quartile\", udf_quartile()) \\\n",
    "    .withColumn(\"country\", udf_country()) \\\n",
    "    .withColumn(\"impact_score\", udf_impact()) \\\n",
    "    .withColumn(\"citations\", (col(\"impact_score\") * 10).cast(IntegerType())) \\\n",
    "    .withColumn(\"generated_keywords\", udf_keywords(col(\"title\"))) \\\n",
    "    .withColumn(\"etl_timestamp\", current_timestamp())\n",
    "\n",
    "# Drop raw columns we don't need\n",
    "df_final = df_dw.drop(\"authors\", \"_id\")\n",
    "\n",
    "print(\"\\n>>> 3. SAVING TO MONGO DW (aci.fact_publications)...\")\n",
    "df_final.write.format(\"mongodb\").mode(\"overwrite\").save()\n",
    "print(\"   - Data written to MongoDB successfully.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. HDFS ARBORESCENCE (The Deliverable)\n",
    "# ==========================================\n",
    "hdfs_output_path = \"hdfs_data\"\n",
    "print(f\"\\n>>> 4. GENERATING HDFS STRUCTURE AT: {os.path.abspath(hdfs_output_path)}\")\n",
    "\n",
    "# Write to Parquet format partitioned by Country\n",
    "df_final.write.mode(\"overwrite\").partitionBy(\"country\").parquet(hdfs_output_path)\n",
    "print(\"   - HDFS Folder Structure Created âœ…\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. ANALYTICS PREVIEW\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"        SPARK ANALYSIS RESULTS        \")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"\\nðŸ“Š 1. Top 5 Countries by Average Impact Factor:\")\n",
    "df_final.groupBy(\"country\") \\\n",
    "    .agg(avg(\"impact_score\").alias(\"avg_impact\"), count(\"title\").alias(\"pub_count\")) \\\n",
    "    .orderBy(desc(\"avg_impact\")) \\\n",
    "    .show(5)\n",
    "\n",
    "print(\"\\nðŸ“Š 2. Publication Count by Quartile:\")\n",
    "df_final.groupBy(\"quartile\").count().orderBy(\"quartile\").show()\n",
    "\n",
    "print(\"\\nâœ… NOTEBOOK EXECUTION COMPLETE.\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c66121-972e-47e9-ad97-8be53c9b48a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
